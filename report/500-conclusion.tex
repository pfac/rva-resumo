\section{Conclusion}

\cite{bleiweiss08} has successfully proven that an irregular and divergent algorithm such as A$\star$, with a small set of tweaks, could effectively obtain speedups running in a GPU architecture, where this characteristics are harmfull.

The author has further developed the pathfinding library applied to the GPU in \cite{bleiweiss09}. \cite{inam10} also optimized the original implementation of th A$\star$ algorithm in the GPU architecture. This library is publicly available in the NVidia website for developpers.

Although the benefits of using the A$\star$ algorithm in a GPU architecture were demonstrated with this document, \cite{johansson09} questions \citeauthor{bleiweiss08}'s decision to perform tests with a large number of agents and a few nodes.
While the scope of author's work is in improving artificial inteligence features using CUDA, in a game environment it is most common to exist a reasonably low number of agents, and thousands of possible points in their routes. This question remains unanswered in \cite{bleiweiss09}.

Although the basis of this work produced proven results, the comparisons performed in the benchmarks are outdated and were slightly bias. Two very distinct CPUs with very small parallel capabilities were used. Due to the already mentioned characteristics of the algorithm, a modern CPU with a high parallel capability should be able to nearly match the results obtained with the GPU hardware described in \cite{bleiweiss08}. On the other hand, GPU hardware has greatly evolved since the article was published. Among other characteristics, GPUs have much more memory, and are able to compute using double precision floating-point numbers.

A more up-to-date approach to this problem can be found in \cite{inam10}, although the authors only explore the implementation of the A$\star$ algorithm in a GPU, instead of the whole pathfinding system.